
# Spark Programming
## package org.apache.spark.examples
SparkPi.scala 有偏无偏
LogQuery
## package org.apache.spark.examples.streaming
- StreamingExamples.scala
一个设置日志级别的示例
```
val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements //获取Logger
    if (!log4jInitialized) {
      // We first log something to initialize Spark's default logging, then we override the
      // logging level.
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.") //打印信息
      Logger.getRootLogger.setLevel(Level.WARN) //设置Logger级别
```
- HdfsWordCount.scala、NetworkWordCount.scala
从HDFS/网络中进行WordCount示例
`    val lines = ssc.textFileStream(args(0))`
`    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)`
- SqlNetworkWordCount.scala 重要 TODO
从网络中读取数据并利用SparkSQL进行查询的示例。
- RecoverableNetworkWordCount.scala 重要 TODO
`val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args` TODO
- StatefulNetworkWordCount.scala 重要 TODO
`val stateDstream = wordDstream.mapWithState(StateSpec.function(mappingFunc).initialState(initialRDD))`
- FlumeEventCount.scala
`val stream = FlumeUtils.createStream(ssc, host, port, StorageLevel.MEMORY_ONLY_SER_2)`
- FlumePollingEventCount.scala
`   val stream = FlumeUtils.createPollingStream(ssc, host, port)`
- QueueStream.scala 重要 TODO
1. 创建RDD队列
```scala
// Create the queue through which RDDs can be pushed to
    // a QueueInputDStream
    val rddQueue = new Queue[RDD[Int]]()
```
2. 创建QueueInputDStream
```scala
// Create the QueueInputDStream and use it do some processing
    val inputStream = ssc.queueStream(rddQueue)
```
3. DStream处理
4. 向RDD队列中添加数据 重要
```scala
	// Create and push some RDDs into rddQueue
    for (i <- 1 to 30) {
      rddQueue.synchronized {
        rddQueue += ssc.sparkContext.makeRDD(1 to 1000, 10)
      }
      Thread.sleep(1000)
    }
```
- RawNetworkGrep.scala
```scala
val rawStreams = (1 to numStreams).map(_ =>
      ssc.rawSocketStream[String](host, port, StorageLevel.MEMORY_ONLY_SER_2)).toArray // 创建了numStreams个Streams数组
      val union = ssc.union(rawStreams) //合并rawStreams
```
- DirectKafkaWordCount.scala TODO
- KafkaWordCount.scala TOOD
- CustomReceiver.scala
使用自定义Receiver创建DStream
`  val lines = ssc.receiverStream(new CustomReceiver(args(0), args(1).toInt))`
自定义Receiver
```scala
class CustomReceiver(host: String, port: Int)
  extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging {

  def onStart() {
    // Start the thread that receives data over a connection
    new Thread("Socket Receiver") {
      override def run() { receive() }
    }.start()
  }

  def onStop() {
   // There is nothing much to do as the thread calling receive()
   // is designed to stop by itself isStopped() returns false
  }

  /** Create a socket connection and receive data until receiver is stopped */
  private def receive() {
   var socket: Socket = null
   var userInput: String = null
   try {
     logInfo("Connecting to " + host + ":" + port)
     socket = new Socket(host, port)
     logInfo("Connected to " + host + ":" + port)
     val reader = new BufferedReader(
       new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))
     userInput = reader.readLine()
     while(!isStopped && userInput != null) {
       store(userInput) // Store():Store the bytes of received data as a data block into Spark's memory.
       userInput = reader.readLine()
     }
     reader.close()
     socket.close()
     logInfo("Stopped receiving")
     restart("Trying to connect again")	//restart():Restart the receiver.
   } catch {
     case e: java.net.ConnectException =>
       restart("Error connecting to " + host + ":" + port, e)
     case t: Throwable =>
       restart("Error receiving data", t)
   }
  }
}
```
## package org.apache.spark.examples.streaming.clickstream TOOD

# Spark SQL
## package org.apache.spark.examples.sql
- SparkSQLExample.scala
```scala
	// Print the schema in a tree format
    df.printSchema()
	// Select everybody, but increment the age by 1
    df.select($"name", $"age" + 1).show()
    // Select people older than 21
	// Count people by age
    df.groupBy("age").count().show()
	// Encoders are created for case classes
    val caseClassDS = Seq(Person("Andy", 32)).toDS()
	// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
    val path = "examples/src/main/resources/people.json"
    val peopleDS = spark.read.json(path).as[Person]
    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly
    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
    // Primitive types and case classes can be also defined as
    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
    teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
    // Array(Map("name" -> "Justin", "age" -> 19))
    
    // The schema is encoded in a string
    val schemaString = "name age"

    // Generate the schema based on the string of schema
    val fields = schemaString.split(" ")
      .map(fieldName => StructField(fieldName, StringType, nullable = true))
    val schema = StructType(fields)

    // Convert records of the RDD (people) to Rows
    val rowRDD = peopleRDD
      .map(_.split(","))
      .map(attributes => Row(attributes(0), attributes(1).trim))

    // Apply the schema to the RDD
    val peopleDF = spark.createDataFrame(rowRDD, schema)
```
- RDDRelation.scala
```scala
val df = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
// Aggregation queries are also supported.
    val count = spark.sql("SELECT COUNT(*) FROM records").collect().head.getLong(0)
    // Queries can also be written using a LINQ-like Scala DSL.
    df.where($"key" === 1).orderBy($"value".asc).select($"key").collect().foreach(println)
```
- SQLDataSourceExample.scala
```scala
	val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
    peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet")
    val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
    
```



